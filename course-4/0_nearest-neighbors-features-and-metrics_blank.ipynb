{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring a large set of documents -- such as Wikipedia, news articles, StackOverflow, etc. -- it can be useful to get a list of related material. To find relevant documents you typically\n",
    "* Decide on a notion of similarity\n",
    "* Find the documents that are most similar \n",
    "\n",
    "In the assignment you will\n",
    "* Gain intuition for different notions of similarity and practice finding similar documents. \n",
    "* Explore the tradeoffs with representing documents using raw word counts and TF-IDF\n",
    "* Explore the behavior of different distance metrics by looking at the Wikipedia pages most similar to President Obama’s page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to Amazon EC2 users**: To conserve memory, make sure to stop all the other notebooks before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we need to first import the Python packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    \n",
    "    return csr_matrix( (data, indices, indptr), shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same dataset of Wikipedia pages that we used in the Machine Learning Foundations course (Course 1). Each element of the dataset consists of a link to the wikipedia article, the name of the person, and the text of the article (in lowercase).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59071, 547979)\n"
     ]
    }
   ],
   "source": [
    "word_count = load_sparse_csr('data/people_wiki_word_count.npz')\n",
    "print(word_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59071, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \n",
       "0  digby morrell born 10 october 1979 is a former...  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2  harpdog brown is a singer and harmonica player...  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = pd.read_csv('data/people_wiki.csv')\n",
    "print(wiki.shape)\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547979\n"
     ]
    }
   ],
   "source": [
    "with open('data/people_wiki_map_index_to_word.json', 'r') as f:\n",
    "    map_index_to_word = json.load(f)\n",
    "print(len(map_index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inv_dict(d):\n",
    "    rd = {}\n",
    "    for k, v in d.items():\n",
    "        assert v not in rd\n",
    "        rd[v] = k\n",
    "    return rd\n",
    "inv_map_index_to_word = inv_dict(map_index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bioarchaeologist', 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_map_index_to_word[0], map_index_to_word['bioarchaeologist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract word count vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in Course 1, we can extract word count vectors using a GraphLab utility function.  We add this as a column in `wiki`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by finding the nearest neighbors of the Barack Obama page using the word count vectors to represent the articles and Euclidean distance to measure distance.  For this, again will we use a GraphLab Create implementation of nearest neighbor search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', leaf_size=30, metric='euclidean',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=5, p=2, radius=1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NearestNeighbors(metric='euclidean', algorithm='brute')\n",
    "model.fit(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the top 10 nearest neighbors by performing the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35817</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Barack_Obama&gt;</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>barack hussein obama ii brk husen bm born augu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              URI          name  \\\n",
       "35817  <http://dbpedia.org/resource/Barack_Obama>  Barack Obama   \n",
       "\n",
       "                                                    text  \n",
       "35817  barack hussein obama ii brk husen bm born augu...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki[wiki['name'] == 'Barack Obama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances, indices = model.kneighbors(word_count[35817], n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unpack_dict(matrix, inv_map_index_to_word):\n",
    "    table = inv_map_index_to_word\n",
    "    data = matrix.data\n",
    "    indices = matrix.indices\n",
    "    indptr = matrix.indptr\n",
    "\n",
    "    num_doc = matrix.shape[0]\n",
    "\n",
    "    wd = []\n",
    "    for i in range(num_doc):\n",
    "        r = []\n",
    "        for word_id in indices[indptr[i]:indptr[i+1]]:\n",
    "            r.append(table[word_id])\n",
    "        di = dict(zip(r, data[indptr[i]:indptr[i+1]].tolist()))\n",
    "        wd.append(di)\n",
    "    return wd\n",
    "\n",
    "wiki['word_count'] = unpack_dict(word_count, inv_map_index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35817</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Barack_Obama&gt;</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>barack hussein obama ii brk husen bm born augu...</td>\n",
       "      <td>{'republicans': 1, 'us': 6, 'repeal': 1, 'spen...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24478</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Joe_Biden&gt;</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>joseph robinette joe biden jr dosf rbnt badn b...</td>\n",
       "      <td>{'republicans': 1, 'us': 5, 'spending': 1, 'fi...</td>\n",
       "      <td>33.075671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28447</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/George_W._Bush&gt;</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>george walker bush born july 6 1946 is an amer...</td>\n",
       "      <td>{'no': 1, 'drug': 1, 'controversial': 1, 'fewe...</td>\n",
       "      <td>34.394767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35357</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Lawrence_Summers&gt;</td>\n",
       "      <td>Lawrence Summers</td>\n",
       "      <td>lawrence henry larry summers born november 30 ...</td>\n",
       "      <td>{'us': 1, 'university': 3, 'actfollowing': 1, ...</td>\n",
       "      <td>36.152455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14754</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Mitt_Romney&gt;</td>\n",
       "      <td>Mitt Romney</td>\n",
       "      <td>willard mitt romney born march 12 1947 is an a...</td>\n",
       "      <td>{'spending': 1, 'us': 2, 'becoming': 1, '33220...</td>\n",
       "      <td>36.166283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13229</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Francisco_Barrio&gt;</td>\n",
       "      <td>Francisco Barrio</td>\n",
       "      <td>francisco javier barrio terrazas born november...</td>\n",
       "      <td>{'controversial': 1, 'university': 1, 'walked'...</td>\n",
       "      <td>36.331804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31423</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Walter_Mondale&gt;</td>\n",
       "      <td>Walter Mondale</td>\n",
       "      <td>walter frederick fritz mondale born january 5 ...</td>\n",
       "      <td>{'republicans': 1, 'us': 3, 'university': 1, '...</td>\n",
       "      <td>36.400549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22745</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Wynn_Normington_H...</td>\n",
       "      <td>Wynn Normington Hugh-Jones</td>\n",
       "      <td>sir wynn normington hughjones kb sometimes kno...</td>\n",
       "      <td>{'way': 1, 'union': 1, 'honours': 1, 'other': ...</td>\n",
       "      <td>36.496575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36364</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Don_Bonker&gt;</td>\n",
       "      <td>Don Bonker</td>\n",
       "      <td>don leroy bonker born march 7 1937 in denver c...</td>\n",
       "      <td>{'us': 2, 'democrat': 3, 'kramer': 1, 'congres...</td>\n",
       "      <td>36.633318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9210</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Andy_Anstett&gt;</td>\n",
       "      <td>Andy Anstett</td>\n",
       "      <td>andrue john andy anstett born june 25 1946 is ...</td>\n",
       "      <td>{'riding': 1, 'university': 1, 'honours': 1, '...</td>\n",
       "      <td>36.959437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     URI  \\\n",
       "35817         <http://dbpedia.org/resource/Barack_Obama>   \n",
       "24478            <http://dbpedia.org/resource/Joe_Biden>   \n",
       "28447       <http://dbpedia.org/resource/George_W._Bush>   \n",
       "35357     <http://dbpedia.org/resource/Lawrence_Summers>   \n",
       "14754          <http://dbpedia.org/resource/Mitt_Romney>   \n",
       "13229     <http://dbpedia.org/resource/Francisco_Barrio>   \n",
       "31423       <http://dbpedia.org/resource/Walter_Mondale>   \n",
       "22745  <http://dbpedia.org/resource/Wynn_Normington_H...   \n",
       "36364           <http://dbpedia.org/resource/Don_Bonker>   \n",
       "9210          <http://dbpedia.org/resource/Andy_Anstett>   \n",
       "\n",
       "                             name  \\\n",
       "35817                Barack Obama   \n",
       "24478                   Joe Biden   \n",
       "28447              George W. Bush   \n",
       "35357            Lawrence Summers   \n",
       "14754                 Mitt Romney   \n",
       "13229            Francisco Barrio   \n",
       "31423              Walter Mondale   \n",
       "22745  Wynn Normington Hugh-Jones   \n",
       "36364                  Don Bonker   \n",
       "9210                 Andy Anstett   \n",
       "\n",
       "                                                    text  \\\n",
       "35817  barack hussein obama ii brk husen bm born augu...   \n",
       "24478  joseph robinette joe biden jr dosf rbnt badn b...   \n",
       "28447  george walker bush born july 6 1946 is an amer...   \n",
       "35357  lawrence henry larry summers born november 30 ...   \n",
       "14754  willard mitt romney born march 12 1947 is an a...   \n",
       "13229  francisco javier barrio terrazas born november...   \n",
       "31423  walter frederick fritz mondale born january 5 ...   \n",
       "22745  sir wynn normington hughjones kb sometimes kno...   \n",
       "36364  don leroy bonker born march 7 1937 in denver c...   \n",
       "9210   andrue john andy anstett born june 25 1946 is ...   \n",
       "\n",
       "                                              word_count   distance  \n",
       "35817  {'republicans': 1, 'us': 6, 'repeal': 1, 'spen...   0.000000  \n",
       "24478  {'republicans': 1, 'us': 5, 'spending': 1, 'fi...  33.075671  \n",
       "28447  {'no': 1, 'drug': 1, 'controversial': 1, 'fewe...  34.394767  \n",
       "35357  {'us': 1, 'university': 3, 'actfollowing': 1, ...  36.152455  \n",
       "14754  {'spending': 1, 'us': 2, 'becoming': 1, '33220...  36.166283  \n",
       "13229  {'controversial': 1, 'university': 1, 'walked'...  36.331804  \n",
       "31423  {'republicans': 1, 'us': 3, 'university': 1, '...  36.400549  \n",
       "22745  {'way': 1, 'union': 1, 'honours': 1, 'other': ...  36.496575  \n",
       "36364  {'us': 2, 'democrat': 3, 'kramer': 1, 'congres...  36.633318  \n",
       "9210   {'riding': 1, 'university': 1, 'honours': 1, '...  36.959437  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wiki.loc[indices[0]].copy()\n",
    "x['distance'] = distances[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the 10 people are politicians, but about half of them have rather tenuous connections with Obama, other than the fact that they are politicians.\n",
    "\n",
    "* Francisco Barrio is a Mexican politician, and a former governor of Chihuahua.\n",
    "* Walter Mondale and Don Bonker are Democrats who made their career in late 1970s.\n",
    "* Wynn Normington Hugh-Jones is a former British diplomat and Liberal Party official.\n",
    "* Andy Anstett is a former politician in Manitoba, Canada.\n",
    "\n",
    "Nearest neighbors with raw word counts got some things right, showing all politicians in the query result, but missed finer and important details.\n",
    "\n",
    "For instance, let's find out why Francisco Barrio was considered a close neighbor of Obama.  To do this, let's look at the most frequently used words in each of Barack Obama and Francisco Barrio's pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_words(name):\n",
    "    \"\"\"\n",
    "    Get a table of the most frequent words in the given person's wikipedia page.\n",
    "    \"\"\"\n",
    "    row = wiki[wiki['name'] == name]\n",
    "    word_count_table = pd.DataFrame(row['word_count'].values.tolist()).T.rename(columns={0:'count'})\n",
    "    return word_count_table.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_obama</th>\n",
       "      <th>count_barrio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>control</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>party</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>states</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>january</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>university</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presidential</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>november</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>born</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>during</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>served</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ending</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>july</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lost</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regained</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>against</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>made</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rights</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>months</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>received</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>process</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worked</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>won</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>while</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>before</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>federal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count_obama  count_barrio\n",
       "the                    40            36\n",
       "in                     30            17\n",
       "and                    21            18\n",
       "of                     18            24\n",
       "to                     14             9\n",
       "his                    11             5\n",
       "he                      7            10\n",
       "a                       7             6\n",
       "as                      6             5\n",
       "was                     5             4\n",
       "for                     4             3\n",
       "president               4             1\n",
       "control                 4             2\n",
       "has                     4             1\n",
       "from                    3             4\n",
       "party                   3             3\n",
       "states                  3             1\n",
       "2009                    3             1\n",
       "with                    3             1\n",
       "term                    3             1\n",
       "january                 3             1\n",
       "first                   3             3\n",
       "national                2             3\n",
       "university              2             1\n",
       "on                      2             1\n",
       "presidential            2             1\n",
       "november                2             1\n",
       "is                      2             2\n",
       "at                      2             2\n",
       "born                    2             2\n",
       "during                  2             2\n",
       "served                  2             2\n",
       "ending                  1             1\n",
       "july                    1             1\n",
       "lost                    1             1\n",
       "regained                1             1\n",
       "degree                  1             1\n",
       "against                 1             1\n",
       "made                    1             1\n",
       "1992                    1             1\n",
       "rights                  1             1\n",
       "review                  1             1\n",
       "months                  1             1\n",
       "received                1             1\n",
       "process                 1             1\n",
       "worked                  1             1\n",
       "won                     1             1\n",
       "while                   1             1\n",
       "where                   1             1\n",
       "named                   1             1\n",
       "not                     1             1\n",
       "by                      1             2\n",
       "before                  1             2\n",
       "years                   1             3\n",
       "state                   1             4\n",
       "federal                 1             1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_words = top_words('Barack Obama')\n",
    "barrio_words = top_words('Francisco Barrio')\n",
    "combined_words = obama_words.merge(barrio_words, left_index=True, right_index=True, suffixes=['_obama', '_barrio'])\n",
    "combined_words.sort_values(by='count_obama', ascending=False, inplace=True)\n",
    "combined_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Among the words that appear in both Barack Obama and Francisco Barrio, take the 5 that appear most frequently in Obama. How many of the articles in the Wikipedia dataset contain all of those 5 words?\n",
    "\n",
    "Hint:\n",
    "* Refer to the previous paragraph for finding the words that appear in both articles. Sort the common words by their frequencies in Obama's article and take the largest five.\n",
    "* Each word count vector is a Python dictionary. For each word count vector in SFrame, you'd have to check if the set of the 5 common words is a subset of the keys of the word count vector. Complete the function `has_top_words` to accomplish the task.\n",
    "  - Convert the list of top 5 words into set using the syntax\n",
    "```\n",
    "set(common_words)\n",
    "```\n",
    "    where `common_words` is a Python list. See [this link](https://docs.python.org/2/library/stdtypes.html#set) if you're curious about Python sets.\n",
    "  - Extract the list of keys of the word count dictionary by calling the [`keys()` method](https://docs.python.org/2/library/stdtypes.html#dict.keys).\n",
    "  - Convert the list of keys into a set as well.\n",
    "  - Use [`issubset()` method](https://docs.python.org/2/library/stdtypes.html#set) to check if all 5 words are among the keys.\n",
    "* Now apply the `has_top_words` function on every row of the SFrame.\n",
    "* Compute the sum of the result column to obtain the number of articles containing all the 5 top words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'to', 'of', 'and', 'in'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9491290142371045, 56066)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = set(combined_words.head(5).index.tolist())\n",
    "print(common_words)\n",
    "def has_top_words(word_count_vector):\n",
    "    # extract the keys of word_count_vector and convert it to a set\n",
    "    unique_words = set(word_count_vector.keys())   # YOUR CODE HERE\n",
    "    # return True if common_words is a subset of unique_words\n",
    "    # return False otherwise\n",
    "    return common_words.issubset(unique_words)  # YOUR CODE HERE\n",
    "\n",
    "wiki['has_top_words'] = wiki['word_count'].apply(has_top_words)\n",
    "\n",
    "wiki['has_top_words'].mean(), wiki['has_top_words'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. Check your `has_top_words` function on two random articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from your function: True\n",
      "Correct output: True\n",
      "Also check the length of unique_words. It should be 167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Output from your function:', has_top_words(wiki.loc[32]['word_count']))\n",
    "print('Correct output: True')\n",
    "print('Also check the length of unique_words. It should be 167')\n",
    "len(set(wiki.loc[32, 'word_count'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from your function: False\n",
      "Correct output: False\n",
      "Also check the length of unique_words. It should be 188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Output from your function:', has_top_words(wiki.iloc[33]['word_count']))\n",
    "print('Correct output: False')\n",
    "print('Also check the length of unique_words. It should be 188')\n",
    "len(set(wiki.loc[33, 'word_count'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Measure the pairwise distance between the Wikipedia pages of Barack Obama, George W. Bush, and Joe Biden. Which of the three pairs has the smallest distance?\n",
    "\n",
    "Hint: To compute the Euclidean distance between two dictionaries, use `graphlab.toolkits.distances.euclidean`. Refer to [this link](https://dato.com/products/create/docs/generated/graphlab.toolkits.distances.euclidean.html) for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_dict(d1, d2):\n",
    "    d = {k:0 for k in d1.keys()}\n",
    "    d.update({k:0 for k in d2.keys()})\n",
    "    ds = 0\n",
    "    v1, v2 = np.zeros(len(d)), np.zeros(len(d))\n",
    "    for i, w in enumerate(d.keys()):\n",
    "        ds += ((d1.get(w, 0) - d2.get(w, 0))**2)\n",
    "        v1[i] = d1.get(w, 0)\n",
    "        v2[i] = d2.get(w, 0)\n",
    "    return np.sqrt(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.756679</td>\n",
       "      <td>32.756679</td>\n",
       "      <td>(Joe Biden, George W. Bush)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.075671</td>\n",
       "      <td>33.075671</td>\n",
       "      <td>(Joe Biden, Barack Obama)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.394767</td>\n",
       "      <td>34.394767</td>\n",
       "      <td>(George W. Bush, Barack Obama)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1                               2\n",
       "0  32.756679  32.756679     (Joe Biden, George W. Bush)\n",
       "1  33.075671  33.075671       (Joe Biden, Barack Obama)\n",
       "2  34.394767  34.394767  (George W. Bush, Barack Obama)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean, cosine\n",
    "names = ('Barack Obama', 'George W. Bush', 'Joe Biden')\n",
    "idxs = wiki[wiki.name.isin(names)].index.values\n",
    "distances = []\n",
    "for i in range(len(idxs)):\n",
    "    for j in range(i+1, len(idxs)):\n",
    "        idx_i, idx_j = idxs[i], idxs[j]\n",
    "        v1 = word_count[idx_i].toarray()\n",
    "        v2 = word_count[idx_j].toarray()\n",
    "        dist = euclidean(v1, v2)\n",
    "        dist2 = euclidean_dict(wiki.loc[idx_i, 'word_count'], wiki.loc[idx_j, 'word_count'])\n",
    "        distances.append((dist2, dist, (wiki.loc[idx_i, 'name'], wiki.loc[idx_j, 'name'])))\n",
    "pd.DataFrame(distances).sort_values(by=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Collect all words that appear both in Barack Obama and George W. Bush pages.  Out of those words, find the 10 words that show up most often in Obama's page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_obama</th>\n",
       "      <th>count_bush</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>law</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iraq</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>control</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>democratic</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>response</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>party</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signed</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>often</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>against</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>into</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>administration</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ii</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsuccessfully</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recession</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>while</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worked</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>referred</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reform</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regained</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policies</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>received</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>months</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>before</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promoted</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>care</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                count_obama  count_bush\n",
       "the                      40          39\n",
       "in                       30          22\n",
       "and                      21          14\n",
       "of                       18          14\n",
       "to                       14          11\n",
       "his                      11           6\n",
       "act                       8           3\n",
       "a                         7           6\n",
       "he                        7           8\n",
       "law                       6           1\n",
       "as                        6           6\n",
       "was                       5           5\n",
       "for                       4           5\n",
       "iraq                      4           2\n",
       "has                       4           4\n",
       "control                   4           1\n",
       "after                     4           3\n",
       "president                 4           6\n",
       "military                  4           1\n",
       "democratic                4           2\n",
       "response                  3           1\n",
       "party                     3           1\n",
       "with                      3           1\n",
       "2004                      3           1\n",
       "school                    3           1\n",
       "signed                    3           1\n",
       "campaign                  3           1\n",
       "american                  3           1\n",
       "2009                      3           2\n",
       "term                      3           1\n",
       "...                     ...         ...\n",
       "often                     1           1\n",
       "great                     1           1\n",
       "john                      1           2\n",
       "against                   1           1\n",
       "into                      1           2\n",
       "administration            1           1\n",
       "2013                      1           1\n",
       "2008                      1           1\n",
       "2007                      1           1\n",
       "ii                        1           1\n",
       "unsuccessfully            1           1\n",
       "which                     1           1\n",
       "recession                 1           2\n",
       "while                     1           1\n",
       "then                      1           1\n",
       "worked                    1           1\n",
       "tax                       1           1\n",
       "referred                  1           1\n",
       "reform                    1           1\n",
       "regained                  1           1\n",
       "close                     1           2\n",
       "new                       1           1\n",
       "policies                  1           1\n",
       "other                     1           1\n",
       "received                  1           2\n",
       "months                    1           1\n",
       "before                    1           1\n",
       "by                        1           1\n",
       "promoted                  1           1\n",
       "care                      1           1\n",
       "\n",
       "[86 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_words = top_words('Barack Obama')\n",
    "bush_words = top_words('George W. Bush')\n",
    "combined_words = obama_words.merge(bush_words, left_index=True, right_index=True, suffixes=['_obama', '_bush'])\n",
    "combined_words.sort_values(by='count_obama', ascending=False, inplace=True)\n",
    "combined_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note.** Even though common words are swamping out important subtle differences, commonalities in rarer political words still matter on the margin. This is why politicians are being listed in the query result instead of musicians, for example. In the next subsection, we will introduce a different metric that will place greater emphasis on those rarer words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the perceived commonalities between Obama and Barrio were due to occurrences of extremely frequent words, such as \"the\", \"and\", and \"his\". So nearest neighbors is recommending plausible results sometimes for the wrong reasons. \n",
    "\n",
    "To retrieve articles that are more relevant, we should focus more on rare words that don't happen in every article. **TF-IDF** (term frequency–inverse document frequency) is a feature representation that penalizes words that are too common.  Let's use GraphLab Create's implementation of TF-IDF and repeat the search for the 10 nearest neighbors of Barack Obama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki['tf_idf'] = graphlab.text_analytics.tf_idf(wiki['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_tf_idf = graphlab.nearest_neighbors.create(wiki, label='name', features=['tf_idf'],\n",
    "                                                 method='brute_force', distance='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_tf_idf.query(wiki[wiki['name'] == 'Barack Obama'], label='name', k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine whether this list makes sense.\n",
    "* With a notable exception of Roland Grossenbacher, the other 8 are all American politicians who are contemporaries of Barack Obama.\n",
    "* Phil Schiliro, Jesse Lee, Samantha Power, and Eric Stern worked for Obama.\n",
    "\n",
    "Clearly, the results are more plausible with the use of TF-IDF. Let's take a look at the word vector for Obama and Schilirio's pages. Notice that TF-IDF representation assigns a weight to each word. This weight captures relative importance of that word in the document. Let us sort the words in Obama's article by their TF-IDF weights; we do the same for Schiliro's article as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_words_tf_idf(name):\n",
    "    row = wiki[wiki['name'] == name]\n",
    "    word_count_table = row[['tf_idf']].stack('tf_idf', new_column_name=['word','weight'])\n",
    "    return word_count_table.sort('weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obama_tf_idf = top_words_tf_idf('Barack Obama')\n",
    "obama_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schiliro_tf_idf = top_words_tf_idf('Phil Schiliro')\n",
    "schiliro_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **join** operation we learned earlier, try your hands at computing the common words shared by Obama's and Schiliro's articles. Sort the common words by their TF-IDF weights in Obama's document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 10 words should say: Obama, law, democratic, Senate, presidential, president, policy, states, office, 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Among the words that appear in both Barack Obama and Phil Schiliro, take the 5 that have largest weights in Obama. How many of the articles in the Wikipedia dataset contain all of those 5 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_words = ...  # YOUR CODE HERE\n",
    "\n",
    "def has_top_words(word_count_vector):\n",
    "    # extract the keys of word_count_vector and convert it to a set\n",
    "    unique_words = ...   # YOUR CODE HERE\n",
    "    # return True if common_words is a subset of unique_words\n",
    "    # return False otherwise\n",
    "    return ...  # YOUR CODE HERE\n",
    "\n",
    "wiki['has_top_words'] = wiki['word_count'].apply(has_top_words)\n",
    "\n",
    "# use has_top_words column to answer the quiz question\n",
    "...  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the huge difference in this calculation using TF-IDF scores instead  of raw word counts. We've eliminated noise arising from extremely common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why Joe Biden, Obama's running mate in two presidential elections, is missing from the query results of `model_tf_idf`. Let's find out why. First, compute the distance between TF-IDF features of Obama and Biden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Compute the Euclidean distance between TF-IDF features of Obama and Biden. Hint: When using Boolean filter in SFrame/SArray, take the index 0 to access the first match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance is larger than the distances we found for the 10 nearest neighbors, which we repeat here for readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_tf_idf.query(wiki[wiki['name'] == 'Barack Obama'], label='name', k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But one may wonder, is Biden's article that different from Obama's, more so than, say, Schiliro's? It turns out that, when we compute nearest neighbors using the Euclidean distances, we unwittingly favor short articles over long ones. Let us compute the length of each Wikipedia document, and examine the document lengths for the 100 nearest neighbors to Obama's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_length(row):\n",
    "    return len(row['text'])\n",
    "\n",
    "wiki['length'] = wiki.apply(compute_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_euclidean = model_tf_idf.query(wiki[wiki['name'] == 'Barack Obama'], label='name', k=100)\n",
    "nearest_neighbors_euclidean = nearest_neighbors_euclidean.join(wiki[['name', 'length']], on={'reference_label':'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_euclidean.sort('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how these document lengths compare to the lengths of other documents in the corpus, let's make a histogram of the document lengths of Obama's 100 nearest neighbors and compare to a histogram of document lengths for all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.hist(wiki['length'], 50, color='k', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='Entire Wikipedia', zorder=3, alpha=0.8)\n",
    "plt.hist(nearest_neighbors_euclidean['length'], 50, color='r', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (Euclidean)', zorder=10, alpha=0.8)\n",
    "plt.axvline(x=wiki['length'][wiki['name'] == 'Barack Obama'][0], color='k', linestyle='--', linewidth=4,\n",
    "           label='Length of Barack Obama', zorder=2)\n",
    "plt.axvline(x=wiki['length'][wiki['name'] == 'Joe Biden'][0], color='g', linestyle='--', linewidth=4,\n",
    "           label='Length of Joe Biden', zorder=1)\n",
    "plt.axis([1000, 5500, 0, 0.004])\n",
    "\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.title('Distribution of document length')\n",
    "plt.xlabel('# of words')\n",
    "plt.ylabel('Percentage')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to the rest of Wikipedia, nearest neighbors of Obama are overwhemingly short, most of them being shorter than 2000 words. The bias towards short articles is not appropriate in this application as there is really no reason to  favor short articles over long articles (they are all Wikipedia articles, after all). Many Wikipedia articles are 2500 words or more, and both Obama and Biden are over 2500 words long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Both word-count features and TF-IDF are proportional to word frequencies. While TF-IDF penalizes very common words, longer articles tend to have longer TF-IDF vectors simply because they have more words in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To remove this bias, we turn to **cosine distances**:\n",
    "$$\n",
    "d(\\mathbf{x},\\mathbf{y}) = 1 - \\frac{\\mathbf{x}^T\\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}\n",
    "$$\n",
    "Cosine distances let us compare word distributions of two articles of varying lengths.\n",
    "\n",
    "Let us train a new nearest neighbor model, this time with cosine distances.  We then repeat the search for Obama's 100 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2_tf_idf = graphlab.nearest_neighbors.create(wiki, label='name', features=['tf_idf'],\n",
    "                                                  method='brute_force', distance='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_cosine = model2_tf_idf.query(wiki[wiki['name'] == 'Barack Obama'], label='name', k=100)\n",
    "nearest_neighbors_cosine = nearest_neighbors_cosine.join(wiki[['name', 'length']], on={'reference_label':'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_cosine.sort('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a glance at the above table, things look better.  For example, we now see Joe Biden as Barack Obama's nearest neighbor!  We also see Hillary Clinton on the list.  This list looks even more plausible as nearest neighbors of Barack Obama.\n",
    "\n",
    "Let's make a plot to better visualize the effect of having used cosine distance in place of Euclidean on our TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.hist(wiki['length'], 50, color='k', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='Entire Wikipedia', zorder=3, alpha=0.8)\n",
    "plt.hist(nearest_neighbors_euclidean['length'], 50, color='r', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (Euclidean)', zorder=10, alpha=0.8)\n",
    "plt.hist(nearest_neighbors_cosine['length'], 50, color='b', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (cosine)', zorder=11, alpha=0.8)\n",
    "plt.axvline(x=wiki['length'][wiki['name'] == 'Barack Obama'][0], color='k', linestyle='--', linewidth=4,\n",
    "           label='Length of Barack Obama', zorder=2)\n",
    "plt.axvline(x=wiki['length'][wiki['name'] == 'Joe Biden'][0], color='g', linestyle='--', linewidth=4,\n",
    "           label='Length of Joe Biden', zorder=1)\n",
    "plt.axis([1000, 5500, 0, 0.004])\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.title('Distribution of document length')\n",
    "plt.xlabel('# of words')\n",
    "plt.ylabel('Percentage')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the 100 nearest neighbors using cosine distance provide a sampling across the range of document lengths, rather than just short articles like Euclidean distance provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moral of the story**: In deciding the features and distance measures, check if they produce results that make sense for your particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem with cosine distances: tweets vs. long articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily ever after? Not so fast. Cosine distances ignore all document lengths, which may be great in certain situations but not in others. For instance, consider the following (admittedly contrived) example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+--------------------------------------------------------+\n",
    "|                                             +--------+ |\n",
    "|  One that shall not be named                | Follow | |\n",
    "|  @username                                  +--------+ |\n",
    "|                                                        |\n",
    "|  Democratic governments control law in response to     |\n",
    "|  popular act.                                          |\n",
    "|                                                        |\n",
    "|  8:05 AM - 16 May 2016                                 |\n",
    "|                                                        |\n",
    "|  Reply   Retweet (1,332)   Like (300)                  |\n",
    "|                                                        |\n",
    "+--------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar is this tweet to Barack Obama's Wikipedia article? Let's transform the tweet into TF-IDF features, using an encoder fit to the Wikipedia dataset.  (That is, let's treat this tweet as an article in our Wikipedia dataset and see what happens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf = graphlab.SFrame({'text': ['democratic governments control law in response to popular act']})\n",
    "sf['word_count'] = graphlab.text_analytics.count_words(sf['text'])\n",
    "\n",
    "encoder = graphlab.feature_engineering.TFIDF(features=['word_count'], output_column_prefix='tf_idf')\n",
    "encoder.fit(wiki)\n",
    "sf = encoder.transform(sf)\n",
    "sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the TF-IDF vectors for this tweet and for Barack Obama's Wikipedia entry, just to visually see their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_tf_idf = sf[0]['tf_idf.word_count']\n",
    "tweet_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the cosine distance between the Barack Obama article and this tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obama = wiki[wiki['name'] == 'Barack Obama']\n",
    "obama_tf_idf = obama[0]['tf_idf']\n",
    "graphlab.toolkits.distances.cosine(obama_tf_idf, tweet_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this distance to the distance between the Barack Obama article and all of its Wikipedia 10 nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2_tf_idf.query(obama, label='name', k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cosine distances, the tweet is \"nearer\" to Barack Obama than everyone else, except for Joe Biden!  This probably is not something we want. If someone is reading the Barack Obama Wikipedia page, would you want to recommend they read this tweet? Ignoring article lengths completely resulted in nonsensical results. In practice, it is common to enforce maximum or minimum document lengths. After all, when someone is reading a long article from _The Atlantic_, you wouldn't recommend him/her a tweet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
